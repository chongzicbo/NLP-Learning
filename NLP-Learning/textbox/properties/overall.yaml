# general
gpu_id: 0
use_gpu: True
seed: 2020
state: INFO
reproducibility: True
data_path: 'dataset/'
wandb: 'online'

# training settings
epochs: 50
train_batch_size: 16
optimizer: adamw
learning_rate: 3e-5
valid_strategy: epoch
valid_steps: 1
stopping_steps: 2
grad_clip: 0.1
optimizer_kwargs: {}
accumulation_steps: 1
adafactor_kwargs: {'lr': 1e-3, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}

# evaluation settings
metrics: ["bleu"]
eval_batch_size: 64

# Pretrained model settings
config_kwargs: {}
tokenizer_kwargs: {'use_fast': True} #'additional_special_tokens': None
tokenizer_add_tokens: []
generation_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}
# efficient_methods: ['lora']
# efficient_kwargs: {'lora_r': 4, 'lora_dropout': 0.1, 'lora_alpha': 32}
# efficient_methods: ['prefix-tuning'] # ['p-tuning-v2']
# efficient_kwargs: {'prefix_length': 100, 'prefix_dropout': 0.1, 'prefix_mid_dim': 512}
# efficient_methods: ['adapter']
# efficient_kwargs: {'adapter_mid_dim': 64}
# efficient_methods: ['prompt-tuning']
# efficient_kwargs: {'prompt_length': 100}
# efficient_methods: ['bitfit']
# efficient_kwargs: {}
efficient_methods: []
efficient_kwargs: {}
efficient_unfreeze_model: False
label_smoothing: 0.1

# Evaluation settings
lower_evaluation: True
multiref_strategy: max

bleu_max_ngrams: 4
bleu_type: nltk
smoothing_function: 0
corpus_bleu: False

rouge_max_ngrams: 2
rouge_type: files2rouge

meteor_type: pycocoevalcap
corpus_meteor: True

chrf_type: m-popovic

distinct_max_ngrams: 4
inter_distinct: True

unique_max_ngrams: 4

self_bleu_max_ngrams: 4

# Dataset Parameters
tgt_lang: 'en'
truncate: tail