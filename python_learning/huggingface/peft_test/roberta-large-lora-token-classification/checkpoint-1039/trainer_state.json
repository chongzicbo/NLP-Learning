{
  "best_metric": 0.18281608819961548,
  "best_model_checkpoint": "roberta-large-lora-token-classification/checkpoint-1039",
  "epoch": 1.0,
  "global_step": 1039,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.48,
      "learning_rate": 0.0009518768046198268,
      "loss": 0.177,
      "step": 500
    },
    {
      "epoch": 0.96,
      "learning_rate": 0.0009037536092396536,
      "loss": 0.1714,
      "step": 1000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9433928852510625,
      "eval_f1": 0.7720994475138122,
      "eval_loss": 0.18281608819961548,
      "eval_precision": 0.7415022384347538,
      "eval_recall": 0.8053304520079236,
      "eval_runtime": 9.6367,
      "eval_samples_per_second": 199.966,
      "eval_steps_per_second": 12.556,
      "step": 1039
    }
  ],
  "max_steps": 10390,
  "num_train_epochs": 10,
  "total_flos": 2355357148018524.0,
  "trial_name": null,
  "trial_params": null
}
