{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0417d7-201c-4fd0-8949-ca90b5502ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/data/bocheng/data/.cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/data/bocheng/data/.cache/huggingface/hub/\"\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d2eaf6-1d18-406f-a22a-fc135efa07c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.62s/it]\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0. All model parameters: 3533967360 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:  <s> Write me a poem about Singapore. Unterscheidung between the two is not always clear-cut, as both countries share a common history and cultural heritage. The poem explores the themes of identity, belonging, and the search for meaning in a rapidly changing world.\n",
      "The poem also touches on the idea of\n",
      "Write me a poem about Singapore. Unterscheidung between the two is not always clear-cut, as both countries share a common history and cultural heritage. The poem explores the themes of identity, belonging, and the search for meaning in a rapidly changing world.\n",
      "The poem also touches on the idea of\n"
     ]
    }
   ],
   "source": [
    "### config ###\n",
    "model_id = \"llama-2-7b-chat-guanaco\"  # optional meta-llama/Llama-2–7b-chat-hf\n",
    "max_length = 512\n",
    "device_map = \"auto\"\n",
    "batch_size = 128\n",
    "micro_batch_size = 32\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "# nf4\" use a symmetric quantization scheme with 4 bits precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# load model from huggingface\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, use_cache=False, device_map=device_map\n",
    ")\n",
    "\n",
    "\n",
    "# load tokenizer from huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable model parameters: {trainable_model_params}. All model parameters: {all_model_params} \"\n",
    "    )\n",
    "    return trainable_model_params\n",
    "\n",
    "\n",
    "ori_p = print_number_of_trainable_model_parameters(model)\n",
    "# LoRA config\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "### compare trainable parameters #\n",
    "# peft_p = print_number_of_trainable_model_parameters(model)\n",
    "# print(\n",
    "#     f\"# Trainable Parameter \\nBefore: {ori_p} \\nAfter: {peft_p} \\nPercentage: {round(peft_p / ori_p * 100, 2)}\"\n",
    "# )\n",
    "### generate ###\n",
    "prompt = \"Write me a poem about Singapore.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=64)\n",
    "print(\"\\nAnswer: \", tokenizer.decode(generate_ids[0]))\n",
    "res = tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc958217-6285-4cda-a2a6-11b20fa0b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:  <s> Write me a poem about Spring. Unterscheidung between the two seasons.\n",
      "\n",
      "Spring is a time of renewal and growth,\n",
      "A season of hope and new beginnings.\n",
      "The snow melts away, and flowers bloom,\n",
      "Bringing color and life to the room.\n",
      "\n",
      "Summer is a season of warmth and fun,\n",
      "A time for adventure and play.\n",
      "The sun shines bright, and the days are long,\n",
      "Bringing joy and laughter to the throng.\n",
      "\n",
      "Autumn is a season of change and harvest,\n",
      "A time for gathering and thanksgiving.\n",
      "The leaves turn golden, and the air is crisp,\n",
      "Bringing a sense of peace and quiet.\n",
      "\n",
      "Winter is a season of rest and peace,\n",
      "A time for dreams and quiet release.\n",
      "The snow falls soft, and the world is still,\n",
      "Bringing a sense of calm and quiet thrill.\n",
      "\n",
      "Each season brings its own unique charm,\n",
      "A time for joy, and a time for alarm.\n",
      "But in the end, they all come together,\n",
      "To bring us closer to each other.</s>\n",
      "Write me a poem about Spring. Unterscheidung between the two seasons.\n",
      "\n",
      "Spring is a time of renewal and growth,\n",
      "A season of hope and new beginnings.\n",
      "The snow melts away, and flowers bloom,\n",
      "Bringing color and life to the room.\n",
      "\n",
      "Summer is a season of warmth and fun,\n",
      "A time for adventure and play.\n",
      "The sun shines bright, and the days are long,\n",
      "Bringing joy and laughter to the throng.\n",
      "\n",
      "Autumn is a season of change and harvest,\n",
      "A time for gathering and thanksgiving.\n",
      "The leaves turn golden, and the air is crisp,\n",
      "Bringing a sense of peace and quiet.\n",
      "\n",
      "Winter is a season of rest and peace,\n",
      "A time for dreams and quiet release.\n",
      "The snow falls soft, and the world is still,\n",
      "Bringing a sense of calm and quiet thrill.\n",
      "\n",
      "Each season brings its own unique charm,\n",
      "A time for joy, and a time for alarm.\n",
      "But in the end, they all come together,\n",
      "To bring us closer to each other.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write me a poem about Spring.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=500)\n",
    "print(\"\\nAnswer: \", tokenizer.decode(generate_ids[0]))\n",
    "res = tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa669a0-faf7-4133-9a01-3ac4824cefce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████| 14011/14011 [00:18<00:00, 740.88 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 676.44 examples/s]\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 08:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.417700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.562400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.386900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.349500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "dataset = datasets.load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "### generate prompt based on template ###\n",
    "prompt_template = {\n",
    "    \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context.\\\n",
    "    Write a response that appropriately completes the request.\\\n",
    "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "    \"prompt_no_input\": \"Below is an instruction that describes a task.\\\n",
    "    Write a response that appropriately completes the request.\\\n",
    "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "    \"response_split\": \"### Response:\",\n",
    "}\n",
    "\n",
    "\n",
    "def generate_prompt(\n",
    "    instruction, input=None, label=None, prompt_template=prompt_template\n",
    "):\n",
    "    if input:\n",
    "        res = prompt_template[\"prompt_input\"].format(\n",
    "            instruction=instruction, input=input\n",
    "        )\n",
    "    else:\n",
    "        res = prompt_template[\"prompt_no_input\"].format(instruction=instruction)\n",
    "    if label:\n",
    "        res = f\"{res}{label}\"\n",
    "    return res\n",
    "\n",
    "\n",
    "def tokenize(tokenizer, prompt, max_length=max_length, add_eos_token=False):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"context\"],\n",
    "        data_point[\"response\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(tokenizer, full_prompt)\n",
    "    user_prompt = generate_prompt(data_point[\"instruction\"], data_point[\"context\"])\n",
    "    tokenized_user_prompt = tokenize(tokenizer, user_prompt)\n",
    "    user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "    mask_token = [-100] * user_prompt_len\n",
    "    tokenized_full_prompt[\"labels\"] = (\n",
    "        mask_token + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n",
    "    )\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "cols = [\"instruction\", \"context\", \"response\", \"category\"]\n",
    "train_data = (\n",
    "    dataset[\"train\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols)\n",
    ")\n",
    "val_data = (\n",
    "    dataset[\"test\"]\n",
    "    .shuffle()\n",
    "    .map(\n",
    "        generate_and_tokenize_prompt,\n",
    "        remove_columns=cols,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./llama-7b-int4-dolly\",\n",
    "    num_train_epochs=20,\n",
    "    max_steps=200,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    # max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=args,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "\n",
    "# silence the warnings. re-enable for inference!\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "model.save_pretrained(\"llama-7b-int4-dolly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101083d8-ce79-4fed-bbc4-a2f027bb8133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.62s/it]\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data/bocheng/soft/installed/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:  <s> Write me a poem about Spring. Unterscheidung between the seasons.\n",
      "Spring is a time of renewal and growth,\n",
      "A time when the earth awakens from its winter sleep.\n",
      "The snow melts away, the flowers bloom,\n",
      "And the world is filled with new life.\n",
      "\n",
      "The birds sing sweet melodies,\n",
      "As they build their nests and raise their young.\n",
      "The air is filled with the scent of blooming flowers,\n",
      "And the warmth of the sun on your skin.\n",
      "\n",
      "Spring is a time of hope and promise,\n",
      "A time when anything is possible.\n",
      "It's a time when the world awakens from its winter slumber,\n",
      "And new life springs forth.\n",
      "\n",
      "Spring is a time of renewal and growth,\n",
      "A time when the earth awakens from its winter sleep.\n",
      "The snow melts away, the flowers bloom,\n",
      "And the world is filled with new life.\n",
      "\n",
      "Spring is a time of hope and promise,\n",
      "A time when anything is possible.\n",
      "It's a time when the world awakens from its winter slumber,\n",
      "And new life springs forth.\n",
      "\n",
      "Spring is a time of renewal and growth,\n",
      "A time when the earth awakens from its winter sleep.\n",
      "The snow melts away, the flowers bloom,\n",
      "And the world is filled with new life.\n",
      "\n",
      "Spring is a time of hope and promise,\n",
      "A time when anything is possible.\n",
      "It's a time when the world awakens from its winter slumber,\n",
      "And new life springs forth.\n",
      "\n",
      "Spring is a time of renewal and growth,\n",
      "A time when the earth awakens from its winter sleep.\n",
      "The snow melts away, the flowers bloom,\n",
      "And the world is filled with new life.\n",
      "\n",
      "Spring is a time of hope and promise,\n",
      "A time when anything is possible.\n",
      "It's a time when the world awakens from its winter slumber,\n",
      "And new life springs forth.\n",
      "\n",
      "Spring is a time of renewal and growth,\n",
      "A time when the earth awakens from its winter sleep.\n",
      "The snow melts away, the flowers bloom,\n",
      "And the world is filled with new life.\n",
      "\n",
      "Spring is a time of hope and promise,\n",
      "A time when anything is possible.\n",
      "It's a time when the world awakens from its winter slumber,\n",
      "And new\n"
     ]
    }
   ],
   "source": [
    "model_id = \"llama-2-7b-chat-guanaco\" \n",
    "peft_path = \"./llama-7b-int4-dolly\"\n",
    "\n",
    "# loading model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# loading peft weight\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    peft_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# generation config\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    # top_k=40,\n",
    "    num_beams=4,  # beam search\n",
    ")\n",
    "\n",
    "# generating reply\n",
    "with torch.no_grad():\n",
    "    prompt = \"Write me a poem about Spring.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    generation_output = model.generate(\n",
    "        input_ids=inputs.input_ids.cuda(),\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    print(\"\\nAnswer: \", tokenizer.decode(generation_output.sequences[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b932e2-6c82-4b22-b8a7-7bcdbf8ee736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
